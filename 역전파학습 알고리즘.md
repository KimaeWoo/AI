# 역전파학습 알고리즘

생성 일시: 2025년 1월 8일 오후 1:12

> 역전파(backpropagation)는 인공신경망에서 사용되는 학습 알고리즘으로, 예측값과 실제값 간의 오차를 기반으로 가중치를 업데이트하는 데 사용됩니다. 주로 오차 역전파와 경사하강법(Gradient Descent)을 결합하여 모델을 학습시킵니다.
> 

### 주요 단계

1. **순방향 전파(Forward Propagation):**
    - 입력 데이터가 네트워크를 통해 계산됩니다.
    - 각 층에서의 계산 결과는 다음 층으로 전달되며 최종 출력값을 얻습니다.
2. **오차 계산(Error Calculation):**
    - 출력값과 목표값(실제값) 간의 차이를 오차로 계산합니다.
3. **오차 역전파(Backward Propagation):**
    - 출력층에서 시작해 이전 층으로 오차를 역으로 전파합니다.
    - 각 노드에서의 기여도를 계산하여 오차를 분배합니다.
4. **가중치 업데이트(Weight Update):**
    - 계산된 오차를 기반으로 가중치를 업데이트합니다.
    - 일반적으로 경사하강법을 사용하여 손실 함수의 기울기에 따라 가중치를 조정합니다.

```python
# 역전파 학습 알고리즘
import numpy as np

# 시그모이드 함수
# 주어진 입력 값에 대해 출력값을 0과 1 사이의 값으로 변환
# 각 뉴런이 활성화되는 정도를 결정하는 데 사용
def actf(x):
    return 1 / (1 + np.exp(-x))

# 시그모이드 함수의 미분값
def actf_deriv(x):
    return x * (1 - x)

# XOR 연산을 위한 4행 * 2열의 입력 행렬
# 마지막 열은 바이어스를 나타낸다.
X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])

# XOR 연산을 위한 4행 * 1열의 목표 행렬
y = np.array([[0], [1], [1], [0]])
np.random.seed(5)
inputs = 3      # 입력층의 노드 개수
hiddens = 6     # 은닝층의 노드 개수
outputs = 1     # 출력층의 노드 개수

# 가중치를 -1.0에서 1.0 사이의 난수로 초기화한다.
weight0 = 2 * np.random.random((inputs, hiddens)) - 1
weight1 = 2 * np.random.random((hiddens, outputs)) - 1

# 반복
for i in range(10000):
    # 순방향 계산
    layer0 = X                     # 입력을 layer0에 대입
    net1 = np.dot(layer0, weight0) # 행렬의 곱을 계산
    layer1 = actf(net1)            # 활성화 함수를 적용
    # 바이어스 항을 활성화 함수에 입력된 값으로 포함
    layer1[:,-1] = 1.0             # 마지막 열은 바이어스를 나타낸다. 1.0으로 만든다.
    net2 = np.dot(layer1, weight1) # 행렬의 곱을 계산한다.
    layer2 = actf(net2)            # 활성화 함수를 적용한다.

    # 출력층에서의 오차를 계산한다.
    layer2_error = layer2 - y

    # 출력층에서의 델타값을 계산한다.
    # 델타는 가중치를 조정하는 데 필요한 오차의 크기
    layer2_delta = layer2_error * actf_deriv(layer2)
    # 은닉층에서의 오차를 계산한다.
    # 여기서 T는 행렬의 전치를 의미한다.
    # 역방향으로 오차를 전파할 때는 반대방향이므로 행렬이 전치되어야 한다.
    layer1_error = np.dot(layer2_delta, weight1.T)

    # 은닉층에서의 델타를 계산한다.
    layer1_delta = layer1_error * actf_deriv(layer1)
    # 은닉층 -> 출력층을 연결하는 가중치를 수정한다.
    weight1 += -0.2 * np.dot(layer1.T, layer2_delta)
    # 입력층 -> 은닉층을 연결하는 가중치를 수정한다.
    weight0 += -0.2 * np.dot(layer0.T, layer1_delta)

print(layer2)  # 현재 출력층의 값을 출력한다.
```

### **1. 초기값 설정**

```python
X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])  # 입력 데이터 (XOR 문제)
y = np.array([[0], [1], [1], [0]])                 # 목표값
np.random.seed(5)                                  # 난수 고정
inputs, hiddens, outputs = 3, 6, 1                 # 입력, 은닉, 출력 노드 개수

weight0 = 2 * np.random.random((inputs, hiddens)) - 1  # 입력층 -> 은닉층 가중치 초기화
weight1 = 2 * np.random.random((hiddens, outputs)) - 1 # 은닉층 -> 출력층 가중치 초기화
```

- 입력 데이터 `X`와 목표값 `y`는 XOR 연산에 해당합니다.
- `np.random.seed(5)`는 **난수 생성기의 초기값**을 5로 설정하는 것입니다. 즉, `numpy`의 난수 생성기에서 5라는 시드를 주면 그 이후에 생성되는 난수는 항상 동일한 값이 됩니다.
- `weight0`과 `weight1`은 각 층 사이의 가중치로, 초기값은 -1에서 1 사이의 난수입니다.
- 왜 -1에서 1 사이로 설정할까?
    - **가중치 초기화**는 신경망의 성능에 중요한 영향을 미칩니다. 가중치가 지나치게 크거나 작으면 학습이 제대로 이루어지지 않을 수 있기 때문에, 가중치는 보통 **작고 랜덤한 값으로 초기화**됩니다.
    - 가중치를 -1과 1 사이의 값으로 설정하는 이유는, **시작값이 너무 크지 않게** 하고 **대칭적인 초기화**가 되도록 하기 위해서입니다. 이는 대체로 **학습이 수렴**하는 데 유리한 방식으로 알려져 있습니다.
    
- `np.random.random((inputs, hiddens))`는 **입력층에서 은닉층으로 가는 가중치 행렬**을 생성하는 코드입니다. 이 행렬은 **입력층 노드 수** × **은닉층 노드 수** 크기입니다. 이 행렬의 각 요소는 0과 1 사이의 랜덤한 값으로 초기화됩니다.

### **2. 순방향 계산 (Forward Propagation)**

```python
layer0 = X                     # 입력 데이터
net1 = np.dot(layer0, weight0) # 입력층 -> 은닉층 계산
layer1 = actf(net1)            # 은닉층 활성화 함수 적용
# 바이어스 항을 활성화 함수에 입력된 값으로 포함
layer1[:,-1] = 1.0             # 은닉층 바이어스 설정
net2 = np.dot(layer1, weight1) # 은닉층 -> 출력층 계산
layer2 = actf(net2)            # 출력층 활성화 함수 적용
```

- `np.dot`은 행렬 곱을 수행하여 각 노드의 입력 값을 계산합니다.
- 활성화 함수 `actf`는 시그모이드 함수로, 비선형성을 추가하여 학습을 가능하게 합니다.

### **3. 출력층 오차 계산**

```python
layer2_error = layer2 - y                # 출력층 오차 계산
# 델타는 가중치를 조정하는 데 필요한 오차의 크기
layer2_delta = layer2_error * actf_deriv(layer2)  # 출력층 델타 계산
```

- 출력값(`layer2`)과 실제값(`y`)의 차이로 오차를 계산합니다.
- 출력층 델타는 오차와 활성화 함수의 미분값(`actf_deriv`)을 곱한 값입니다.

### **4. 은닉층 오차 계산**

```python
layer1_error = np.dot(layer2_delta, weight1.T)  # 은닉층 오차 계산
layer1_delta = layer1_error * actf_deriv(layer1)  # 은닉층 델타 계산
```

- `layer1_error`는 출력층 델타와 가중치를 통해 역방향으로 계산된 오차입니다.
- `layer1_delta`는 오차에 활성화 함수의 미분값을 곱해 구합니다.

### **5. 가중치 업데이트**

```python
weight1 += -0.2 * np.dot(layer1.T, layer2_delta)  # 은닉층 -> 출력층 가중치 업데이트
weight0 += -0.2 * np.dot(layer0.T, layer1_delta)  # 입력층 -> 은닉층 가중치 업데이트
```

- `np.dot`으로 각 층 간의 가중치를 업데이트합니다.
- 학습률(learning rate)은 `0.2`로 설정되어 오차를 천천히 줄입니다.

---

역전파는 **신경망**에서 학습을 하는 방법인데, 신경망을 훈련시키려면 가중치를 조정해야 하죠. 그 과정에서 **오차(error)**를 줄여나가는 방법이 바로 역전파입니다.

### 신경망의 기본 구조

먼저 신경망의 구조를 이해해야 합니다. 신경망은 여러 층으로 구성되어 있습니다:

- **입력층**: 데이터를 받는 층입니다. 예를 들어, XOR 문제에서는 2개의 입력값이 있죠.
- **은닉층(hidden layer)**: 입력층에서 받은 정보를 처리하는 중간 층입니다. 이 층은 데이터를 변형하거나 특징을 추출합니다.
- **출력층(output layer)**: 신경망이 예측한 결과를 출력하는 층입니다.

### 예시

예를 들어, XOR 문제에서 `X = [0, 1]`이라고 할 때, 이 값을 입력층에 넣고 신경망을 통해 최종적으로 결과를 출력하는 방식이죠. XOR 문제는 출력이 `1`인지 `0`인지를 예측하는 문제입니다.

### 역전파가 왜 필요한가요?

신경망을 학습시키려면 **가중치(weight)**를 조정해야 합니다. 가중치는 입력값을 변환하는 값입니다. 초기에는 이 가중치가 무작위로 설정되기 때문에 예측이 잘못될 수 있습니다.

그래서 **오차**를 계산하고, 그 오차를 기반으로 가중치를 수정해야 합니다. 오차를 줄이는 방법이 **경사하강법(Gradient Descent)**이고, 이를 실제로 적용하기 위한 알고리즘이 **역전파(backpropagation)**입니다.

### 역전파 과정

역전파는 크게 두 가지 단계로 나눌 수 있습니다:

### 1. **순방향 전파(Forward Propagation)**

- 입력 데이터가 네트워크에 들어가서 출력값을 계산합니다.
- 예를 들어, XOR에서 `[0, 1]`을 입력하면 출력값을 예측하게 됩니다. 처음에는 무작위 가중치로 예측되기 때문에 오차가 있을 수 있습니다.

### 2. **오차 계산 후 역방향 전파(Backward Propagation)**

- 출력값을 계산한 후 **오차**를 계산합니다. 오차는 **예측값과 실제값의 차이**입니다.
- 오차를 각 층으로 **역전파**하여, 각 층의 가중치를 조정합니다. 오차가 가장 큰 부분부터 차례대로 수정하면서 가중치를 업데이트합니다.

### 순방향 계산 (Forward Pass)

먼저 입력값을 통해 예측값을 계산합니다. 이 값은 실제값과 비교되어 **오차**를 계산하는 데 사용됩니다.

예를 들어, `XOR` 문제에서 입력값 `[1, 0]`이 들어가면, 신경망을 통해 예측된 값이 나오겠죠. 하지만 그 예측값은 목표값과 차이가 있을 수 있습니다. 차이를 **오차**라고 합니다.

### 순방향 예시

1. 입력층에서 `[0, 1]`을 받습니다.
2. 입력값을 **가중치**와 곱하여 **은닉층**을 계산합니다.
3. 은닉층에서 또 계산하고, **출력층**에 결과를 보냅니다.
4. 결과값을 얻고, 그 결과와 실제값(목표값)을 비교해 오차를 구합니다.

### 역방향 계산 (Backward Pass)

1. **출력층에서의 오차**: 먼저 출력층에서 오차를 구합니다. 예측값과 목표값의 차이를 계산한 후, 이를 기반으로 가중치를 업데이트할 방법을 찾습니다.
2. **은닉층으로 오차 전파**: 출력층에서 구한 오차를 바탕으로 은닉층의 가중치도 수정해야 합니다. 왜냐하면 은닉층도 출력값에 영향을 주기 때문이죠. 이때 **오차를 거꾸로 전파**하여 각 층의 가중치를 조정합니다.
3. **가중치 업데이트**: 각 층의 가중치를 **조정**합니다. 경사하강법을 사용하여 오차가 줄어들 수 있도록 가중치를 업데이트합니다. 이 과정을 반복하면서 점점 더 정확한 예측을 하게 됩니다.

### 코드에서의 역전파

여기서 코드로 구현된 역전파를 조금 더 구체적으로 설명할게요.

1. **순방향 계산**: 입력값을 각 층을 통해 계산하여 최종 출력값을 얻습니다.
    
    ```python
    layer0 = X                     # 입력 데이터
    net1 = np.dot(layer0, weight0)  # 입력 -> 은닉층
    layer1 = actf(net1)             # 은닉층 활성화 함수
    net2 = np.dot(layer1, weight1)  # 은닉층 -> 출력층
    layer2 = actf(net2)             # 출력층 활성화 함수
    ```
    
2. **오차 계산**: 예측한 값과 목표값의 차이를 계산합니다.
    
    ```python
    layer2_error = layer2 - y  # 예측값과 목표값의 차이
    ```
    
3. **역방향 전파**: 출력층에서 은닉층으로 오차를 전파하여 각 층의 가중치를 업데이트합니다.
    
    ```python
    layer2_delta = layer2_error * actf_deriv(layer2)  # 출력층 델타
    layer1_error = np.dot(layer2_delta, weight1.T)  # 은닉층 오차
    layer1_delta = layer1_error * actf_deriv(layer1)  # 은닉층 델타
    ```
    
4. **가중치 업데이트**: 가중치를 경사하강법을 통해 업데이트합니다.
    
    ```python
    weight1 += -0.2 * np.dot(layer1.T, layer2_delta)  # 출력층 가중치 업데이트
    weight0 += -0.2 * np.dot(layer0.T, layer1_delta)  # 은닉층 가중치 업데이트
    ```
    

### 최종 목표

역전파를 반복적으로 실행하여 **가중치**를 조정하면, 신경망은 점점 더 정확한 예측을 할 수 있습니다. 이 과정을 **훈련**이라고 하며, 훈련이 끝난 후에는 예측이 매우 정확해지게 됩니다.

---

### 요약

- **순방향 전파(Forward Propagation)**: 입력값을 네트워크에 넣어 예측값을 계산합니다.
- **오차 계산**: 예측값과 실제값의 차이를 계산합니다.
- **역전파(Backpropagation)**: 오차를 역으로 전파하여 가중치를 업데이트합니다.
- **경사하강법**: 오차가 최소화되도록 가중치를 수정합니다.

이 과정을 여러 번 반복하여 신경망이 점점 더 좋은 성능을 발휘하도록 학습시킬 수 있습니다.

### 문제1) layer2_error를 그래프로 나타내시오

```python
import numpy as np
import matplotlib.pyplot as plt

# XOR 입력과 목표값
X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
y = np.array([[0], [1], [1], [0]])

# 랜덤으로 가중치 초기화
np.random.seed(5)
weight0 = 2 * np.random.random((3, 6)) - 1
weight1 = 2 * np.random.random((6, 1)) - 1

# 시그모이드 함수 및 미분
def actf(x):
    return 1 / (1 + np.exp(-x))

# 시그모이드 함수의 미분값
def actf_deriv(x):
    return x * (1 - x)

# 순방향 계산
layer0 = X
net1 = np.dot(layer0, weight0)
layer1 = actf(net1)
layer1[:, -1] = 1.0  # 바이어스 설정
net2 = np.dot(layer1, weight1)
layer2 = actf(net2)

# 출력층에서의 오차 계산
layer2_error = layer2 - y

# 그래프 그리기
plt.plot(layer2_error, marker='o', linestyle='-', color='b')
plt.title("Layer 2 Error")
plt.xlabel("Sample")
plt.ylabel("Error Value")
plt.grid(True)
plt.show()
```

![image](https://github.com/user-attachments/assets/df3b03a7-300a-4e48-87f2-a1665fbb52fa)

### 문제2) 위의 예제를 이용하여 7-세그먼트의 2진입력(예: 0인 경우 1111110-->000)을 받아, 2진 출력을 하는 인공지능을 설계하시오.

```python
import numpy as np

# 시그모이드 함수 및 미분
def actf(x):
    return 1 / (1 + np.exp(-x))

def actf_deriv(x):
    return x * (1 - x)

# 7-segment 표시 (입력 데이터)
X = np.array([
    [1, 1, 1, 1, 1, 1, 0],  # 0
    [0, 1, 1, 0, 0, 0, 0],  # 1
    [1, 1, 0, 1, 1, 0, 1],  # 2
    [1, 1, 1, 1, 0, 0, 1],  # 3
    [0, 1, 1, 0, 0, 1, 1],  # 4
    [1, 0, 1, 1, 0, 1, 1],  # 5
    [1, 0, 1, 1, 1, 1, 1],  # 6
    [1, 1, 1, 0, 0, 0, 0],  # 7
    [1, 1, 1, 1, 1, 1, 1],  # 8
    [1, 1, 1, 1, 0, 1, 1]   # 9
])

# 2진 출력 (각 숫자의 이진 값)
y = np.array([
    [0, 0, 0],  # 0
    [0, 0, 1],  # 1
    [0, 1, 0],  # 2
    [0, 1, 1],  # 3
    [1, 0, 0],  # 4
    [1, 0, 1],  # 5
    [1, 1, 0],  # 6
    [1, 1, 1],  # 7
    [0, 0, 0],  # 8
    [0, 0, 1]   # 9
])

# 랜덤 가중치 초기화
np.random.seed(5)
inputs = 7      # 입력층 노드 수
hiddens = 6     # 은닉층 노드 수
outputs = 3     # 출력층 노드 수

weight0 = 2 * np.random.random((inputs, hiddens)) - 1
weight1 = 2 * np.random.random((hiddens, outputs)) - 1

# 학습
for i in range(10000):
    # 순방향 계산
    layer0 = X
    net1 = np.dot(layer0, weight0)
    layer1 = actf(net1)
    layer1[:,-1] = 1.0  # 바이어스 설정
    net2 = np.dot(layer1, weight1)
    layer2 = actf(net2)
    
    # 출력층 오차 계산
    layer2_error = layer2 - y
    
    # 출력층 델타 계산
    layer2_delta = layer2_error * actf_deriv(layer2)
    
    # 은닉층 오차 계산
    layer1_error = np.dot(layer2_delta, weight1.T)
    
    # 은닉층 델타 계산
    layer1_delta = layer1_error * actf_deriv(layer1)
    
    # 가중치 업데이트
    weight1 += -0.2 * np.dot(layer1.T, layer2_delta)
    weight0 += -0.2 * np.dot(layer0.T, layer1_delta)

# 출력 결과 확인
print(layer2)
```

```
[[0.00752288 0.00998295 0.00857682]
 [0.01241428 0.00636888 0.99118344]
 [0.01198648 0.99870667 0.00892672]
 [0.00909515 0.98819922 0.98656206]
 [0.98402254 0.00181101 0.01698306]
 [0.98877834 0.02095462 0.99307987]
 [0.98913811 0.97834705 0.01157502]
 [0.98389163 0.98906751 0.99771125]
 [0.0135862  0.01934354 0.00233099]
 [0.01934389 0.0078706  0.9752459 ]]
```
