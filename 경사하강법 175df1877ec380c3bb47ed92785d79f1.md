# 경사하강법

생성 일시: 2025년 1월 8일 오전 11:46

- 경사하강법의 필요성
    
    경사하강법(Gradient Descent)은 **최적화 문제**를 해결하기 위해 사용되는 핵심 알고리즘으로, 머신러닝과 딥러닝에서 모델을 학습시키기 위한 중요한 도구입니다. 다음은 경사하강법이 필요한 이유를 단계별로 설명합니다.
    
    ---
    
    ### 1. **최적화 문제란?**
    
    - 머신러닝에서 모델은 주어진 데이터를 가장 잘 설명할 수 있는 **최적의 매개변수(가중치와 바이어스)**를 찾아야 합니다.
    - 이를 위해, **손실 함수(Loss Function)**를 최소화해야 합니다. 손실 함수는 모델의 예측값과 실제 값의 차이를 측정하는 함수입니다.
    - 손실 함수 값이 작을수록 모델의 성능이 좋아지므로, 이 값을 최소화하는 것이 목표입니다.
    
    ---
    
    ### 2. **왜 최적화 알고리즘이 필요한가?**
    
    - 손실 함수는 매개변수에 대한 복잡한 수학적 함수로 표현됩니다.
    - 대부분의 경우, 이 함수는 고차원적이고 비선형적이기 때문에, 수학적으로 직접 최소값(전역 최솟값)을 구하는 것이 불가능하거나 비효율적입니다.
    - 따라서, **경사하강법**과 같은 최적화 알고리즘을 사용하여 점진적으로 최소값에 가까워지도록 매개변수를 조정합니다.
    
    ---
    
    ### 3. **경사하강법이 해결하는 문제**
    
    경사하강법은 손실 함수의 기울기를 활용하여 다음과 같은 문제를 해결합니다.
    
    ### a. **손실 함수 최소화**
    
    - 손실 함수의 그래디언트(기울기)는 현재 위치에서 기울기가 감소하는 방향을 가리킵니다.
    - 경사하강법은 기울기의 반대 방향으로 매개변수를 업데이트하며 손실 값을 점점 줄여갑니다.
    
    ### b. **고차원 문제 처리**
    
    - 손실 함수가 고차원 공간에서도 적용 가능하며, 각 매개변수에 대해 독립적으로 업데이트를 수행합니다.
    
    ### c. **효율적 탐색**
    
    - 복잡한 함수의 전역 최솟값 또는 지역 최솟값을 찾기 위해 많은 계산 없이 효율적으로 접근할 수 있습니다.
    
    ---
    
    ### 4. **경사하강법의 실제 응용**
    
    경사하강법은 머신러닝과 딥러닝에서 다음과 같은 과정에 사용됩니다.
    
    ### a. **모델 학습**
    
    - 딥러닝 모델(예: 신경망)의 가중치를 학습하기 위해 사용됩니다.
    - 예를 들어, 손실 함수로 MSE(Mean Squared Error)를 사용하는 경우, 경사하강법을 통해 예측값과 실제 값 간의 오차를 최소화하는 가중치를 학습합니다.
    
    ### b. **특성 선택**
    
    - 데이터를 분석하여 중요하거나 관련성이 높은 특성을 선택하는 데도 사용됩니다.
    
    ### c. **기타 최적화 문제**
    
    - 로지스틱 회귀, 선형 회귀, SVM, KNN 등 다양한 모델에서 최적의 매개변수 조정을 위해 사용됩니다.
    
    ---
    
    ### 5. **경사하강법의 중요성**
    
    - 데이터와 모델이 복잡해질수록, 최적화 과정은 더욱 어려워집니다.
    - 경사하강법은 이러한 최적화를 간단하면서도 효과적으로 수행할 수 있는 방법을 제공합니다.
    - 딥러닝에서는 손실 함수가 고차원 공간에 있으며, 직접적인 최적화는 불가능하기 때문에 경사하강법이 필수적입니다.
    
    ---
    
    ### 6. **요약**
    
    - 경사하강법은 손실 함수를 최소화하기 위해 매개변수를 점진적으로 업데이트하는 알고리즘입니다.
    - 기울기를 활용해 효율적으로 최적의 값을 찾아갈 수 있습니다.
    - 머신러닝과 딥러닝의 학습 과정에서 모델의 성능을 최적화하기 위한 필수적인 도구입니다.

> 경사하강법(Gradient Descent)은 머신러닝과 최적화 문제에서 널리 사용되는 알고리즘으로, 비용 함수(Cost Function)를 최소화하기 위해 사용됩니다. 모델의 예측과 실제 값 사이의 오차를 줄이는 방향으로 모델의 매개변수(가중치 및 편향 등)를 업데이트하는 과정입니다.
> 

## 1. **경사하강법의 핵심 아이디어**

- 비용 함수 J(θ): 모델의 예측값과 실제 값 간의 차이를 나타내는 함수로, 최소화해야 하는 목표입니다.
- 목표: 비용 함수 J(θ)를 최소화하는 매개변수 θ를 찾는 것.
- 경사하강법은 J(θ)의 기울기(Gradient)를 계산해, 기울기가 작아지는 방향으로 매개변수를 갱신합니다.

## 2. **수식**

- 비용 함수 J(θ)를 매개변수 θ에 대해 미분하면, ∇J(θ)가 나옵니다. 이는 **비용 함수의 기울기**를 의미합니다.
- 매개변수 θ를 다음과 같이 업데이트합니다:
                                                       θ = θ − η ⋅ ∇J(θ)
    - η\etaη: 학습률(learning rate)로, 매개변수를 업데이트하는 크기를 조절하는 하이퍼파라미터.
    - ∇J(θ): 비용 함수의 기울기, 매개변수에 따라 계산.

## 3. **경사하강법의 주요 유형**

1. **배치 경사하강법 (Batch Gradient Descent)**
    - 전체 데이터셋에 대해 기울기를 계산하고 매개변수를 업데이트.
    - 장점: 매개변수 업데이트 방향이 안정적.
    - 단점: 데이터셋이 클 경우 계산 비용이 높음.
2. **확률적 경사하강법 (Stochastic Gradient Descent, SGD)**
    - 데이터셋의 한 샘플마다 기울기를 계산하고 매개변수를 업데이트.
    - 장점: 계산이 빠르고 메모리 효율적.
    - 단점: 업데이트 방향이 불안정하여 수렴 속도가 느릴 수 있음.
3. **미니배치 경사하강법 (Mini-Batch Gradient Descent)**
    - 데이터셋을 작은 배치(batch)로 나누어 기울기를 계산하고 매개변수를 업데이트.
    - 배치 경사하강법과 SGD의 장점을 절충.

## 4. **경사하강법의 과정**

1. **초기화**
    
    매개변수 θ를 임의의 값으로 설정.
    
2. **비용 함수 계산**
    
    모델의 현재 상태에서 비용 함수 J(의 값을 계산.
    
3. **기울기 계산**
    
    비용 함수 J(θ)를 매개변수 θ\thetaθ에 대해 미분하여 기울기 ∇J(θ)를 계산.
    
4. **매개변수 업데이트**
    
    기울기를 이용해 매개변수를 업데이트:
    
                                                           θ = θ − η ⋅ ∇J
    
5. **수렴 여부 확인**
    
    비용 함수가 더 이상 감소하지 않거나 기울기가 0에 가까워지면 학습 종료.
    

## 5. **경사하강법의 한계와 개선 방법**

### 한계

1. **학습률 조정**
    - 학습률이 너무 크면 최적값에 도달하지 못하고 발산.
    - 학습률이 너무 작으면 수렴 속도가 느림.
2. **지역 최적값 문제**
    - 비선형 비용 함수에서는 전역 최적값(Global Optimum)이 아닌 지역 최적값(Local Optimum)에 빠질 수 있음.

### 개선 방법

1. **적응적 학습률(Adaptive Learning Rate)**
    - AdaGrad, RMSProp, Adam 등 학습률을 자동으로 조정하는 기법.
2. **모멘텀(Momentum)**
    - 기울기의 방향을 누적하여, 경사가 완만한 방향으로 더 빠르게 수렴하도록 도움.

주어진 손실 함수의 최소값을 찾는 과정

```python
# 경사하강법
x = 10
learning_rate = 0.01
precision = 0.00001
max_iterations = 100

# 손실 함수를 람다식으로 정의한다.
loss_func = lambda x: (x-3) ** 2 + 10

# 그래디언트를 람다식으로 정의한다. 손실 함수의 1차 미분값이다.
gradient = lambda x: 2 * x - 6

# 그래디언트 하강법
for i in range(max_iterations):
    x = x - learning_rate * gradient(x)
    print("손실 함수값(", x, ") = ", loss_func(x))

print('최소값 = ', x
```

### 1. **손실 함수 정의**

```python
loss_func = lambda x: (x-3) ** 2 + 10
```

- 손실 함수는 f(x)=(x−3)2+10로 정의됩니다.
    
    f(x) = (x-3)^2 + 10
    
- 이 함수는 x=3에서 최소값을 가지며, 값은 10입니다.
- 경사하강법의 목표는 이 손실 함수의 최소값을 찾아내는 것입니다.

---

### 2. **그래디언트 정의**

```python
gradient = lambda x: 2 * x - 6
```

- 그래디언트는 손실 함수 f(x)의 **1차 미분값**을 나타냅니다:
f′(x)=2x−6
- 그래디언트는 현재 x에서의 기울기를 계산하며, x를 조정하는 방향을 알려줍니다.
- 경사하강법은 이 그래디언트를 이용해 x를 업데이트합니다.

---

### 3. **초기값 및 매개변수 설정**

```python
x = 10
learning_rate = 0.01
precision = 0.00001
max_iterations = 100
```

- **초기값 x:** 시작점으로 x=10이 설정됩니다
- **학습률(learning_rate):** 0.01로 설정되어, 매개변수 x를 업데이트할 때의 변화량을 결정합니다.
- **정밀도(precision):** 목표 최소값에 얼마나 가까워지면 멈출지 설정합니다.
- **최대 반복 횟수(max_iterations):** 최대 100번까지 반복하며, 정밀도 조건에 도달하지 못하면 종료합니다.

---

### 4. **경사하강법 알고리즘**

```python
for i in range(max_iterations):
    x = x - learning_rate * gradient(x)
    print("손실 함수값(", x, ") = ", loss_func(x))
```

- **기울기 계산 및 업데이트:**

                                                    x_new=x_current−η⋅f′(x_current)

- η는 학습률(learning_rate)입니다.
- f′(x)는 현재 위치에서의 기울기입니다.
- x는 기울기에 따라 값이 업데이트됩니다.
- 손실 함수값을 출력하여 x가 최소값에 점점 가까워지는 과정을 확인합니다.

### 그래프로 시각화

```python
import matplotlib.pyplot as plt

# 초기값 및 설정
x = 10
learning_rate = 0.01
precision = 0.00001
max_iterations = 100

# 손실 함수와 그래디언트 정의
loss_func = lambda x: (x-3) ** 2 + 10
gradient = lambda x: 2 * x - 6

# 손실 함수 값을 저장할 리스트
loss_values = []

# 경사하강법
for i in range(max_iterations):
    x = x - learning_rate * gradient(x)  # x 업데이트
    current_loss = loss_func(x)         # 현재 손실 함수 값 계산
    loss_values.append(current_loss)    # 리스트에 추가
    print(f"Iteration {i+1}, x = {x:.5f}, Loss = {current_loss:.5f}")

print('최소값 x = ', x)

# 손실 함수 그래프 그리기
plt.plot(range(1, max_iterations + 1), loss_values, linestyle='-', color='r')
plt.title('Loss Function Value Over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Loss Function Value')
plt.grid()
# x축과 y축의 비율을 동일하게 설정
plt.gca().set_aspect('equal', adjustable='box')
plt.show()

```

![image.png](%E1%84%80%E1%85%A7%E1%86%BC%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%84%80%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8%20175df1877ec380c3bb47ed92785d79f1/image.png)