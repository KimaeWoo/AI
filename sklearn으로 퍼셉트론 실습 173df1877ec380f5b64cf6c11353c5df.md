# sklearn으로 퍼셉트론 실습

생성 일시: 2025년 1월 6일 오후 1:42

https://sacko.tistory.com/10

### AND 연산을 학습시키는 퍼셉트론

<aside>

```python
from sklearn.linear_model import Perceptron
# 샘플과 레이블이다.
x = [[0,0],[0,1],[1,0],[1,1]]
y = [0,0,0,1]

# 퍼셉트론을 생성한다. tol는 종료 조건이다. random_state는 난수의 시드이다.
clf = Perceptron(tol=1e-3, random_state=0)

# 학습을 수행한다.
clf.fit(x,y)

#테스트를 수행한다. 
print(clf.predict(x))
```

```python
[0 0 0 1]
```

</aside>

### 1. **데이터 정의**

```python
x = [[0,0],[0,1],[1,0],[1,1]]
y = [0,0,0,1]
```

- **입력 데이터 (`x`)**: 논리 게이트의 입력을 나타냅니다. 각 항목은 2개의 값(0 또는 1)으로 구성된 리스트입니다.
    - 예: `[0, 0], [0, 1], [1, 0], [1, 1]`
- **출력 데이터 (`y`)**: 각 입력에 대한 레이블입니다.
    - `[0, 0, 0, 1]`은 논리 게이트 AND의 출력처럼 동작합니다.
        - AND 연산: 두 값이 모두 `1`일 때만 출력이 `1`, 그렇지 않으면 `0`.

---

### 2. **퍼셉트론 생성**

```python
clf = Perceptron(tol=1e-3, random_state=0)
```

- **`Perceptron`**: Scikit-learn 라이브러리에서 제공하는 퍼셉트론 구현체입니다. 퍼셉트론은 단순한 선형 모델로, 입력 데이터와 가중치의 선형 결합에 따라 출력을 결정합니다.
    - 퍼셉트론 공식: y=step(∑wi​xi​+b)
        
        y=step(∑wixi+b)y = \text{step}(\sum w_i x_i + b)
        
        - www: 가중치
        - bbb: 바이어스
        - xxx: 입력값
        - `step`: 활성화 함수 (결과를 0 또는 1로 변환)

### 주요 매개변수

1. **`tol=1e-3`**: 학습의 종료 조건으로, 모델의 성능 변화가 이 값을 넘지 않으면 학습을 멈춥니다.
2. **`random_state=0`**: 난수 시드를 설정해 초기 가중치 값이 고정됩니다. 이로 인해 매번 동일한 결과를 얻을 수 있습니다.

---

### 3. **학습 수행**

```python
clf.fit(x, y)
```

- **`fit` 메서드**: 입력 데이터 `x`와 출력 데이터 `y`를 사용하여 모델을 학습시킵니다.
    - 학습 과정에서 퍼셉트론은 입력과 출력 간의 관계를 학습하며, 각 입력에 적합한 가중치와 바이어스를 업데이트합니다.
    - 이 예제에서는 AND 논리 연산을 학습하도록 설계되었습니다.

---

### 4. **테스트 수행**

```python
print(clf.predict(x))
```

- **`predict` 메서드**: 학습된 모델을 사용해 입력 데이터 `x`에 대해 예측을 수행합니다.
    - 출력은 `[0, 0, 0, 1]`으로, AND 게이트의 출력과 일치합니다.

---

### 실행 결과

- 출력값:
    
    ```python
    [0 0 0 1]
    ```
    
- 입력 데이터와 동일한 AND 논리 연산의 결과를 정확히 예측했음을 확인할 수 있습니다.

---

### 퍼셉트론의 동작 원리

1. **초기화**: 가중치와 바이어스는 랜덤하게 초기화됩니다.
2. **학습**:
    - 각 데이터 포인트 xi​에 대해 예측을 수행하고 y^​i​와 실제 값 yi​를 비교합니다.
        
        xix_i
        
        y^i\hat{y}_i
        
        yiy_i
        
    - 예측이 틀리면 가중치와 바이어스를 수정하여 다음 예측에서 더 정확하게 작동하도록 합니다.
3. **반복**:
    - 데이터가 완전히 분리 가능(linearly separable)할 때까지, 또는 `tol` 조건이 만족될 때까지 학습을 반복합니다.

---

### 주의사항

- 퍼셉트론은 **선형적으로 분리 가능한 데이터**에 대해 완벽히 학습할 수 있지만, XOR 같은 선형적으로 분리되지 않는 문제는 해결할 수 없습니다.
- 더 복잡한 문제를 해결하려면 다층 퍼셉트론(MLP) 같은 비선형 모델이 필요합니다.

---

### `tol=1e-3`의 의미와 학습 종료 조건

`tol`은 **학습 종료 조건** 중 하나로, 모델이 더 이상 성능을 향상시키지 못할 때 학습을 중단하는 기준입니다. `tol`은 **허용 오차 (tolerance)**를 뜻하며, 퍼셉트론이 다음 두 조건 중 하나를 만족하면 학습이 종료됩니다:

---

### 1. **허용 오차 기준**

`tol=1e-3`은 **가중치 업데이트가 더 이상 의미 있게 개선되지 않는 수준**을 나타냅니다. 구체적으로는:

- **학습 과정**: 퍼셉트론은 각 반복(epoch)마다 데이터를 사용해 가중치와 바이어스를 조정합니다.
    - 예측 y^​와 실제 값 y가 다를 경우:
    w=w+η⋅(y−y^​)⋅x
    여기서 η는 학습률입니다.
        
        y^\hat{y}
        
        yy
        
        w=w+η⋅(y−y^)⋅xw = w + \eta \cdot (y - \hat{y}) \cdot x
        
        η\eta
        
- 반복적으로 업데이트하다가 더 이상 가중치나 바이어스가 유의미하게 변화하지 않을 때, 학습을 멈춥니다.

### `tol`의 역할:

- 가중치 변화량이 tol 이하로 작아지면 학습을 중단합니다.
    
    toltol
    
    - ∥Δw∥<tol\| \Delta w \| < \text{tol}∥Δw∥<tol: Δw는 가중치 업데이트의 크기입니다.
        
        Δw\Delta w
        
    - 즉, 더 이상 모델이 데이터에 적응하지 않는다고 판단합니다.

---

### 2. **에포크 기준 (기본 종료 조건)**

퍼셉트론은 최대 반복 횟수(기본값: `max_iter=1000`)에 도달하면 학습을 중단합니다.

- *`max_iter`*와 **`tol`*의 관계:
    - `tol`은 학습이 일찍 끝날 수 있도록 보조합니다.
    - `tol`을 설정하지 않거나 너무 작게 설정하면, 퍼셉트론은 `max_iter`까지 무조건 학습을 반복합니다.

---

### 예시: `tol=1e-3`

1. 모델 학습을 시작합니다.
2. 매 반복(epoch)마다 데이터를 처리하며 가중치 업데이트가 발생합니다.
3. 업데이트 변화량 ∥Δw∥이 0.001 이하로 작아지면, 모델은 **수렴**했다고 판단하고 학습을 멈춥니다.
    
    ∥Δw∥\| \Delta w \|
    
    0.0010.001
    

---

### `tol` 값에 따른 영향

- **작은 `tol` (예: 1e−51e-51e−5):**
    - 더 정확히 수렴하려 하지만, 시간이 오래 걸릴 수 있습니다.
    - 데이터가 잡음(noise)을 포함하면 오히려 과적합(overfitting) 위험이 있습니다.
- **큰 `tol` (예: 1e−21e-21e−2):**
    - 학습이 빠르게 종료되지만, 모델이 충분히 학습하지 못하고 조기 종료(underfitting)할 수 있습니다.

---

### 학습 종료 조건: 정리

퍼셉트론은 다음 조건 중 하나를 만족할 때 학습을 중단합니다:

1. 가중치 변화량 (Δw)이 `tol` 이하로 감소.
    
    Δw\Delta w
    
2. 반복 횟수(\text{max_iter})에 도달.

`tol=1e-3`은 가중치 변화량이 0.0010.0010.001보다 작을 경우, 학습이 충분히 완료되었다고 간주하는 기준입니다.