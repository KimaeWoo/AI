#ì¸ê³µì§€ëŠ¥ í¼ì…‰íŠ¸ë¡  í”„ë¡œê·¸ë¨

import numpy as np
import matplotlib.pyplot as plt

X = np.array([[0,0], [1,0], [0,1], [1,1]]) # ì…ë ¥ ë°ì´í„°
Y = np.array([-1,1,1,1]) # ë ˆì´ë¸”

# ë°ì´í„° ì‹œê°í™”
plt.scatter(X[0][0], X[0][1], c='red') # x,yì¢Œí‘œì— ë¹¨ê°„ìƒ‰ ì  ê·¸ë¦¬
plt.scatter(X[1][0], X[1][1], c='blue')
plt.scatter(X[2][0], X[2][1], c='blue')
plt.scatter(X[3][0], X[3][1], c='blue')
plt.show()

# ì´ˆê¸° ê°€ì¤‘ì¹˜ (bias, w1, w2)
w = np.array([1., 1., 1.]) # [bias, w1, w2]

# í¼ì…‰íŠ¸ë¡  ì˜ˆì¸¡ì„ ìœ„í•œ forward í•¨ìˆ˜
def forward(x):
    # np.dot : ë‘ ë°°ì—´ ê°„ì˜ ì ê³±(ë‚´ì , dot product)ì„ ê³„ì‚°
    # ì„ í˜• ëª¨ë¸ì˜ í•µì‹¬ ê³„ì‚°ì¸ **ê°€ì¤‘ì¹˜(weight)**ì™€ **íŠ¹ì§•(feature)**ì˜ ê³±ì„ íš¨ê³¼ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´
    # ë‚´ì ì€ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì„ í˜• íšŒê·€(linear regression), í¼ì…‰íŠ¸ë¡ (perceptron), ê·¸ë¦¬ê³  ì‹ ê²½ë§ì˜ ê¸°ë³¸ ê³„ì‚° ë°©ì‹
    return np.dot(x, w[1:]) + w[0]

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict(X):
    # ì¡°ê±´ì— ë”°ë¼ ë°°ì—´ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    # í¼ì…‰íŠ¸ë¡ ì˜ ì˜ˆì¸¡ê°’ì„ ê³„ì‚°
    # np.where(ì¡°ê±´, ì°¸ì¼ ë•Œ ê°’, ê±°ì§“ì¼ ë•Œ ê°’)
    return np.where(forward(X) > 0, 1, -1)
    
print('predict (before traning)', w)

for epoch in range(50):
    # í¼ì…‰íŠ¸ë¡  í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ë¶€ë¶„
    # ì£¼ì–´ì§„ ì…ë ¥ ë°ì´í„° ğ‘‹ì™€ í•´ë‹¹ ë ˆì´ë¸” ğ‘Œì— ëŒ€í•´ **ê°€ì¤‘ì¹˜(weight)**ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •
    # ì´ ê³¼ì •ì€ ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    for x_val, y_val in zip(X, Y):
        update = 0.01 * (y_val - predict(x_val)) # ì˜¤ì°¨ ê³„ì‚°
        w[1:] += update * x_val # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        w[0] += update # bias ì—…ë°ì´íŠ¸

print('predict (after traning)', w)